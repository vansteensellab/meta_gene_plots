import os
import inspect
import re


filename = inspect.getframeinfo(inspect.currentframe()).filename
path = os.path.dirname(os.path.abspath(filename))



def get_all(config, regex_list):
    for name in config['transcripts']:
        for exp in config['window']:
            for regex in regex_list:
                yield(regex.format(outdir=config['outdir'], name=name, exp=exp,
                                   **config['window_param']))



rule all:
    input:
        get_all(config, ['{outdir}/scaled_runmean/{name}-{exp}.txt',
                         '{outdir}/window_runmean/{name}-{exp}-{b}-{a}-{bs}.txt'])

rule scaled:
    input:
        get_all(config, ['{outdir}/scaled_runmean/{name}-{exp}.txt'])


rule window:
    input:
        get_all(config, ['{outdir}/window_runmean/{name}-{exp}-{b}-{a}-{bs}.txt'])


def get_matrix(config, wildcards):
    regex_list = ('%s/matrix/%s-signal.txt.gz',
                  '%s/matrix/%s-control.txt.gz')
    for regex in regex_list:
        yield(regex % wildcards.outdir, wildcards.name)

rule plot_window_matrix:
    input:
        signal='{outdir}/window/{name}-{exp}-signal-window-{up}-{down}-{binsize}.txt.gz',
        control='{outdir}/window/{name}-{exp}-control-window-{up}-{down}-{binsize}.txt.gz',
        bed=lambda wildcards: get_bed(config, wildcards),
        stats=config['statistics']
    output:
        '{outdir}/window_runmean/{name}-{exp}-{up}-{down}-{binsize}.txt'
    params:
        binsize='{binsize}',
        up='{up}',
        signal_name='{exp}_Lmnb2',
        ctrl_name='{exp}_Dam',
        down='{down}'
    shell:
        "{path}/scripts/run_mean.R -s {input.signal}"
        "                          -c {input.control}"
        "                          -bs {params.binsize}"
        "                          -a {params.down}"
        "                          -b {params.up}"
        "                          --stats {input.stats}"
        "                          --bed {input.bed}"
        "                          --exp {params.signal_name}"
        "                          --ctrl {params.ctrl_name}"
        "                          -o {output}"

rule plot_scaled_matrix:
    input:
        signal='{outdir}/scaled_window/{name}-{exp}-signal.txt.gz',
        control='{outdir}/scaled_window/{name}-{exp}-control.txt.gz',
        bed='{outdir}/scaled_window/{name}-scaled-selection.bed',
        stats=config['statistics']
    output:
        '{outdir}/scaled_runmean/{name}-{exp}.txt'
    params:
        binsize=config['scaled_param']['bs'],
        up=config['scaled_param']['b'],
        signal_name='{exp}_Lmnb2',
        ctrl_name='{exp}_Dam',
        down=config['scaled_param']['a'],
        unscaled3=config['scaled_param']['unscaled3'],
        unscaled5=config['scaled_param']['unscaled5'],
        body_length=config['scaled_param']['body_length']
    shell:
        "{path}/scripts/run_mean.R -s {input.signal}"
        "                          -c {input.control}"
        "                          -bs {params.binsize}"
        "                          -a {params.down}"
        "                          -b {params.up}"
        "                          -u3 {params.unscaled3}"
        "                          -u5 {params.unscaled5}"
        "                          --body {params.body_length}"
        "                          --stats {input.stats}"
        "                          --bed {input.bed}"
        "                          --exp {params.signal_name}"
        "                          --ctrl {params.ctrl_name}"
        "                          -o {output}"
        "                          --scaled TRUE"

rule scaled_matrix:
    input:
        window='{outdir}/tracks/{exp}-{signal}.bw',
        bed='{outdir}/scaled_window/{name}-scaled-selection.bed'
    output:
        '{outdir}/scaled_window/{name}-{exp}-{signal}.txt.gz'
    message:
        "extracting matrix of sum {wildcards.signal} signal {params.up}bp "
        "upstream and {params.down}bp downstream of {wildcards.name} TSS's "
        "in bins of {params.binsize}bp."
    params:
        binsize=config['scaled_param']['bs'],
        up=config['scaled_param']['b'],
        down=config['scaled_param']['a'],
        unscaled3=config['scaled_param']['unscaled3'],
        unscaled5=config['scaled_param']['unscaled5'],
        body_length=config['scaled_param']['body_length'],
        missingDataAsZero=lambda wildcards: config['missingDataAsZero'][wildcards.exp],
    threads:
        10
    run:
        cmd = ("computeMatrix scale-regions -S {input.window} "
               "                            -R {input.bed} "
               "                            -p {threads}"
               "                            --sortRegions keep"
               "                            --averageTypeBins sum"
               "                            --binSize={params.binsize}"
               "                            -a {params.down} -b {params.up} "
               "                            --unscaled3prime {params.unscaled3}"
               "                            --unscaled5prime {params.unscaled5}"
               "                            -m {params.body_length}"
               "                            --outFileName {output}")
        if params.missingDataAsZero:
            cmd = ' '.join((cmd, '--missingDataAsZero'))
        shell(cmd)


rule scaled_selection:
    input:
        bed=lambda wildcards: get_bed(config, wildcards)
    output:
        '{outdir}/scaled_window/{name}-scaled-selection.bed'
    params:
        body_length=config['scaled_param']['body_length'],
    shell:
        "awk '{{if ($3 - $2 > {params.body_length}){{print $0}}}}' {input}"
        "    > {output}"

rule compute_matrix:
    input:
        window='{outdir}/tracks/{exp}-{signal}.bw',
        bed=lambda wildcards: get_bed(config, wildcards)
    output:
        '{outdir}/window/{name}-{exp}-{signal}-window-{up}-{down}-{binsize}.txt.gz'
    message:
        "extracting matrix of {wildcards.signal} signal {wildcards.up}bp "
        "upstream and {wildcards.down}bp downstream of {wildcards.name} TSS's "
        "in bins of {wildcards.binsize}bp."
    params:
        binsize='{binsize}',
        up='{up}',
        down='{down}',
        missingDataAsZero=lambda wildcards: config['missingDataAsZero'][wildcards.exp],
    threads:
        10
    run:
        cmd = ("computeMatrix reference-point -S {input.window} "
               "                              -R {input.bed} "
               "                              -p {threads}"
               "                              --averageTypeBins sum"
               "                              --binSize={params.binsize}"
               "                              -a {params.down} -b {params.up} "
               "                              --outFileName {output}")
        if params.missingDataAsZero:
            cmd = ' '.join((cmd, '--missingDataAsZero'))
        shell(cmd)



rule centre_damid_to_bw:
    input:
        bg = '{outdir}/tracks/{exp}-{signal}.txt',
        cs = config['chrom_sizes']
    output:
        bw='{outdir}/tracks/{exp}-{signal}.bw'
    shell:
        "bedGraphToBigWig {input.bg} {input.cs} {output}"


rule centre_damid:
    input:
        window=lambda wildcards: config['window'][wildcards.exp][wildcards.signal],
        cs = config['chrom_sizes']
    output:
        temp('{outdir}/tracks/{exp}-{signal}.txt')
    shell:
        "zcat {input.window} | awk '{{"
        "    mid=($2 + $3)/2;"
        "    printf \"%s\\t%d\\t%d\\t%f\\n\", $1, mid-1, mid, $4"
        "}}' | sort -k1,1 -k2,2n > {output}"



def get_bed(config, wildcards):
    if wildcards.name in config['transcripts'].keys():
        return('%s/selection/%s-fantom-selection.bed' % (wildcards.outdir,
                                                         wildcards.name))
    else:
        return('%s/selection/%s-selection.bed' % (wildcards.outdir,
                                                  wildcards.name))




rule report_tss_selection:
    input:
        lambda wildcards: config['transcripts'][wildcards.name],
        tss="{outdir}/selection/{name}-{g_exp}-tss.txt",
        exp="{outdir}/selection/{name}-{g_exp}-tissue-expr.txt.gz",
        report="%s/scripts/report_tss_selection.Rmd" % (path),
    output:
        html="{outdir}/report/{name}-{g_exp}-selection.html",
        txt="{outdir}/selection/{name}-{g_exp}-selection.txt",
        gff="{outdir}/selection/{name}-{g_exp}-selection.gff",
        bed="{outdir}/selection/{name}-{g_exp}-selection.bed"
    shell:
        "{path}/scripts/make_report.R {input.report} {output.html} "
        "                             {input.tss} {input.exp} {input[0]} "
        "                             {output.txt} {output.gff} {output.bed}"



## If TSS's are closer together, let's take the TSS that is generally most
## highly transcribed. For this we will need to have some information on
## transcription rates across fantom5 dataset.
## might as well also count the number of tissues expressed, since this will
## be used later.


rule tss_global_expression:
    input:
        link="{outdir}/selection/{name}-{g_exp}-link.txt",
        exp=config["tissue_expression"]
    output:
        "{outdir}/selection/{name}-{g_exp}-tissue-expr.txt.gz"
    message:
        "calculating sum of expression normalized over input for each tss "
        "in {wildcards.name} (used to select high expressing TSS's) and "
        "counting number of samples with > 0 expression"
    shell:
        "{path}/scripts/tss_fantom_expression.sh -l {input.link} "
        "                                        -e {input.exp} > {output}"



## For multiple transcripts coming from the same gene, we want to select
## transcription start sites at least 500bp apart.

## select unique transcript start sites which overlap with a cage peak.
## CAGE peaks have at least 1 transcript in one of the tissues.
## (multiple transcripts of same gene can start at same position we don't
## want those).

rule tss_exp_selection:
    input:
        tss="{outdir}/raw_data/{name}-tss.bed.gz",
        exp=lambda wildcards: config["transcript_selection"][wildcards.g_exp]
    output:
        selection="{outdir}/selection/{name}-{g_exp}-tss.txt",
        link="{outdir}/selection/{name}-{g_exp}-link.txt"
    params:
        dist=50
    message:
        "selecting {wildcards.name} transcription start sites which are <  "
        "{params.dist}bp away from peaks in {wildcards.g_exp} and "
        "writing table linking identifiers."
    shell:
        "{path}/scripts/tss_exp_selection_overlap.sh -t {input.tss} "
        "                                            -d {params.dist} "
        "                                            -e {input.exp} "
        "                                            -s {output.selection} "
        "                                            -l {output.link}"



## select all transcript start sites from gff
rule gff_to_tss_bed:
    input:
        lambda wildcards: config["transcripts"][wildcards.name]
    output:
        "{outdir}/raw_data/{name}-tss.bed.gz"
    message:
        "selecting all transcription start sites from {input}"
    shell:
        "{path}/scripts/gff_to_tss_bed.sh {input} > {output}"
